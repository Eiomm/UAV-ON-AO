import base64
from io import BytesIO
import traceback
from PIL import Image
import json
import time
import os
from openai import AsyncOpenAI
from openai import OpenAI
try:
    from dashscope import MultiModalConversation
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    print("[WARNING] dashscope not available, will use OpenAI-compatible API")
import asyncio



def encode_image(image_files):

    base_img = []
    for file in image_files:
        encoded_string = base64.b64encode(file).decode("utf-8")
        base_img.append(encoded_string)
    return base_img


def generate_caption(image_file, temperature=0.7):
    n = len(image_file)
    system_prompt = f"""
            You are an image understanding assistant. Your task is to generate one concise and detailed description per image that focuses on:

            1. **Key objects and their core attributes**  
            - List every object with simple nouns **and** one or two attributes (e.g., â€œyellow slide, medium-sized, plasticâ€; â€œSpringer Spaniel dog, tricolor coatâ€; â€œgas station building, metal canopyâ€).  

            2. **Object quantities and groupings**  
            - Specify number if more than one or if itâ€™s a cluster (e.g., â€œthree childrenâ€, â€œa row of parked carsâ€).  

            3. **Precise spatial relationships**  
            - Describe relative positions, distances or directions (e.g., â€œthe dog sits immediately to the right of the slideâ€, â€œthe building stands in the distant background, slightly left of centerâ€).  

            4. **Object states or actions**  
            - Note any visible activity or condition (e.g., â€œchildren sliding downâ€, â€œpump nozzles hanging idleâ€, â€œcar doors openâ€).  

            5. **Avoid opinions or irrelevancies**  
            - Use plain factual language. Do not include judgments, emotional tone words, or fine-grained internal part details.  

            User will supply {n} images; you must return exactly one well-structured description string for every image(no quotation marks).
            For each image, return exactly only one description item. 
            You are not allowed to return a blank caption string or more than {n} caption for {n} images.
            Return your answer **only** as a JSON array of caption strings (no prose outside the array), like:
            Example output exactly:
            [
                "yellow slide, medium-sized, plastic; slide is in the foreground; children sliding down",
                "......",
                "gas station building, metal canopy; two fuel pumps; pump nozzles hanging idle; canopy in the background"
            ]
            Do not write anything else.
            IMPORTANT INSTRUCTIONS:
            - You must respond **only in valid JSON format**.
            - Do **not** include any extra explanation, commentary, markdown formatting, or newlines.
            - The response must be a **single-line JSON object**, not a stringified object or list.
    """



    captions = []
    
    print(f"\nğŸ“¸ å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°ï¼Œæ•°é‡: {n}")
    
    user_content = []
    for img_b64 in image_file:
            user_content.append({'image': 'data:image/png;base64,' + img_b64})
    messages = [{"role": "system",
                "content": [system_prompt]},
                {'role':'user','content': user_content}]

    try:
        print(f"ğŸŒ è°ƒç”¨é€šä¹‰åƒé—®VL-Max API...")
        response = MultiModalConversation.call(
            api_key="sk-080736fa4a9a4dcca9f4b1bfee5d3fd1",  # æ›¿æ¢ä¸ºä½ çš„ qwen-api
            model='qwen-vl-max',
            messages=messages
        )

        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©ºæˆ–ç»“æ„å¼‚å¸¸
        if not response or "output" not in response:
            raise RuntimeError("DashScope API response is None or missing 'output'")

        # å°è¯•æå–æ–‡æœ¬å†…å®¹
        raw = response["output"]["choices"][0]["message"].content[0]["text"].strip()

        # è§£æ JSON æ ¼å¼
        captions = json.loads(raw)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ{len(captions)}ä¸ªå›¾åƒæè¿°")
        for i, cap in enumerate(captions[:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
            display_cap = cap[:80] + "..." if len(cap) > 80 else cap
            print(f"  å›¾åƒ{i+1}: {display_cap}")

        if len(captions) != len(image_file):
            raise ValueError(
                f"For batch of {len(image_file)} images, expected {len(image_file)} captions but got {len(captions)}:\n{captions}"
            )

        return captions

    except Exception as e:
        print("[generate_caption] Failed to get or parse caption:")
        traceback.print_exc()
        # è¿”å›é»˜è®¤æè¿°ï¼ˆé˜²æ­¢ç¨‹åºå´©æºƒï¼‰
        return ["Description time out. [ServerError]"] * len(image_file)